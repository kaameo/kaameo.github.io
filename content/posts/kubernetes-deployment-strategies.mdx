---
title: "Kubernetes 배포 전략: 안전하고 효율적인 애플리케이션 업데이트"
date: "2024-01-28"
excerpt: "Rolling Update, Blue-Green, Canary 등 Kubernetes의 다양한 배포 전략을 실습과 함께 알아봅니다."
category: "DevOps"
tags: ["Kubernetes", "K8s", "배포 전략", "DevOps", "CI/CD"]
author: "Kaameo"
---

프로덕션 환경에서 애플리케이션을 안전하게 업데이트하는 것은 매우 중요합니다. 
Kubernetes는 다양한 배포 전략을 지원하여 서비스 중단 없이 애플리케이션을 업데이트할 수 있게 해줍니다.

## 배포 전략의 중요성

### 왜 배포 전략이 필요한가?
- **무중단 서비스**: 사용자 경험 저하 방지
- **위험 최소화**: 문제 발생 시 빠른 롤백
- **점진적 롤아웃**: 일부 사용자로 테스트 후 전체 배포
- **자동화**: 수동 작업 최소화로 휴먼 에러 방지

## 1. Rolling Update (롤링 업데이트)

가장 기본적이고 널리 사용되는 배포 전략입니다.

### 작동 방식
```
v1 → v1 → v1 → v1  (현재 버전)
↓
v2 → v1 → v1 → v1  (첫 번째 Pod 업데이트)
↓
v2 → v2 → v1 → v1  (두 번째 Pod 업데이트)
↓
v2 → v2 → v2 → v1  (세 번째 Pod 업데이트)
↓
v2 → v2 → v2 → v2  (모든 Pod 업데이트 완료)
```

### 구현 예제

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # 동시에 생성할 수 있는 추가 Pod 수
      maxUnavailable: 1  # 동시에 사용 불가능한 Pod 수
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web-app
        image: myapp:v1
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

### 업데이트 실행
```bash
# 이미지 업데이트
kubectl set image deployment/web-app web-app=myapp:v2

# 롤아웃 상태 확인
kubectl rollout status deployment/web-app

# 롤아웃 히스토리 확인
kubectl rollout history deployment/web-app

# 이전 버전으로 롤백
kubectl rollout undo deployment/web-app
```

### 장단점
**장점**
- Kubernetes 기본 지원
- 리소스 효율적
- 간단한 설정

**단점**
- 일시적으로 v1과 v2가 공존
- 데이터베이스 스키마 변경 시 주의 필요

## 2. Blue-Green 배포

두 개의 동일한 환경을 사용하여 순간적으로 전환하는 전략입니다.

### 작동 방식
```
[Blue (v1) - Active]  ←── Load Balancer ←── Users
[Green (v2) - Standby]

↓ (테스트 완료 후 전환)

[Blue (v1) - Standby]
[Green (v2) - Active] ←── Load Balancer ←── Users
```

### 구현 예제

```yaml
# blue-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: blue
  template:
    metadata:
      labels:
        app: web-app
        version: blue
    spec:
      containers:
      - name: web-app
        image: myapp:v1
        ports:
        - containerPort: 80

---
# green-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: green
  template:
    metadata:
      labels:
        app: web-app
        version: green
    spec:
      containers:
      - name: web-app
        image: myapp:v2
        ports:
        - containerPort: 80

---
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: web-app
    version: blue  # 현재는 Blue를 가리킴
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
```

### Blue-Green 전환 스크립트
```bash
#!/bin/bash
# blue-green-switch.sh

CURRENT_VERSION=$(kubectl get service web-app-service -o jsonpath='{.spec.selector.version}')
echo "현재 활성 버전: $CURRENT_VERSION"

if [ "$CURRENT_VERSION" = "blue" ]; then
    NEW_VERSION="green"
else
    NEW_VERSION="blue"
fi

echo "새 버전으로 전환: $NEW_VERSION"

# Service selector 업데이트
kubectl patch service web-app-service -p '{"spec":{"selector":{"version":"'$NEW_VERSION'"}}}'

echo "전환 완료!"
```

### 장단점
**장점**
- 즉시 롤백 가능
- 테스트 환경과 프로덕션 환경 동일
- v1과 v2 트래픽 혼재 없음

**단점**
- 리소스 2배 필요
- 상태를 가진 애플리케이션에는 복잡
- 데이터베이스 마이그레이션 조정 필요

## 3. Canary 배포

새 버전을 일부 사용자에게만 노출하여 점진적으로 배포하는 전략입니다.

### 작동 방식
```
95% 트래픽 → v1 (Stable)
5% 트래픽  → v2 (Canary)

↓ (모니터링 후 문제 없으면)

50% 트래픽 → v1
50% 트래픽 → v2

↓ (최종 전환)

100% 트래픽 → v2
```

### Nginx Ingress를 사용한 Canary 배포

```yaml
# stable-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: stable
  template:
    metadata:
      labels:
        app: web-app
        version: stable
    spec:
      containers:
      - name: web-app
        image: myapp:v1
        ports:
        - containerPort: 80

---
# canary-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-app
      version: canary
  template:
    metadata:
      labels:
        app: web-app
        version: canary
    spec:
      containers:
      - name: web-app
        image: myapp:v2
        ports:
        - containerPort: 80

---
# stable-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app-stable
spec:
  selector:
    app: web-app
    version: stable
  ports:
  - port: 80
    targetPort: 80

---
# canary-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app-canary
spec:
  selector:
    app: web-app
    version: canary
  ports:
  - port: 80
    targetPort: 80

---
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"  # 10% 트래픽
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-app-canary
            port:
              number: 80
```

### Flagger를 사용한 자동화된 Canary 배포

```yaml
# flagger-canary.yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: web-app
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  service:
    port: 80
  analysis:
    interval: 1m
    threshold: 10
    maxWeight: 50
    stepWeight: 5
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
      interval: 1m
    - name: request-duration
      thresholdRange:
        max: 500
      interval: 1m
  webhooks:
    - name: acceptance-test
      type: pre-rollout
      url: http://flagger-loadtester.test/
      timeout: 30s
      metadata:
        type: bash
        cmd: "curl -sd 'test' http://web-app-canary.test/health"
```

## 4. A/B Testing 배포

사용자를 특정 기준으로 분류하여 다른 버전을 제공하는 전략입니다.

### Istio를 사용한 A/B Testing

```yaml
# virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: web-app-vs
spec:
  hosts:
  - web-app
  http:
  - match:
    - headers:
        user-group:
          exact: beta-testers
    route:
    - destination:
        host: web-app
        subset: v2
      weight: 100
  - route:
    - destination:
        host: web-app
        subset: v1
      weight: 100

---
# destination-rule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: web-app-dr
spec:
  host: web-app
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

## 5. Shadow (미러링) 배포

실제 트래픽을 새 버전에 복제하여 테스트하는 전략입니다.

```yaml
# istio-mirror.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: web-app-vs
spec:
  hosts:
  - web-app
  http:
  - route:
    - destination:
        host: web-app
        subset: v1
      weight: 100
    mirror:
      host: web-app
      subset: v2
    mirrorPercentage:
      value: 100.0
```

## 배포 전략 선택 가이드

### 상황별 추천 전략

| 상황 | 추천 전략 | 이유 |
|-----|----------|------|
| 일반적인 웹 애플리케이션 | Rolling Update | 간단하고 리소스 효율적 |
| 중요한 프로덕션 서비스 | Blue-Green | 즉시 롤백 가능 |
| 대규모 사용자 서비스 | Canary | 위험 최소화, 점진적 배포 |
| 기능 실험 | A/B Testing | 사용자 그룹별 테스트 |
| 성능 테스트 필요 | Shadow | 실제 트래픽으로 테스트 |

## 모니터링과 자동화

### Prometheus + Grafana 모니터링
```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: web-app-metrics
spec:
  selector:
    matchLabels:
      app: web-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

### 배포 자동화 스크립트
```bash
#!/bin/bash
# deploy.sh

VERSION=$1
DEPLOYMENT="web-app"
NAMESPACE="production"

# 헬스체크 함수
check_health() {
    kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE
    if [ $? -eq 0 ]; then
        echo "✅ 배포 성공!"
    else
        echo "❌ 배포 실패! 롤백 중..."
        kubectl rollout undo deployment/$DEPLOYMENT -n $NAMESPACE
        exit 1
    fi
}

# 배포 실행
echo "🚀 버전 $VERSION 배포 시작..."
kubectl set image deployment/$DEPLOYMENT $DEPLOYMENT=myapp:$VERSION -n $NAMESPACE

# 헬스체크
check_health

# 메트릭 확인
echo "📊 메트릭 확인 중..."
sleep 30

# 에러율 체크 (예시)
ERROR_RATE=$(kubectl exec -n monitoring prometheus-0 -- promtool query instant \
  'rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])' | \
  grep -oP '[\d.]+')

if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
    echo "❌ 에러율이 높습니다 ($ERROR_RATE). 롤백 중..."
    kubectl rollout undo deployment/$DEPLOYMENT -n $NAMESPACE
    exit 1
fi

echo "✅ 배포 완료!"
```

## 베스트 프랙티스

### 1. Health Check 설정
```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 80
  initialDelaySeconds: 10
  periodSeconds: 5
  
livenessProbe:
  httpGet:
    path: /health
    port: 80
  initialDelaySeconds: 30
  periodSeconds: 10
```

### 2. 리소스 제한 설정
```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

### 3. PodDisruptionBudget 설정
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web-app
```

## 마무리

각 배포 전략은 고유한 장단점이 있으며, 애플리케이션의 특성과 비즈니스 요구사항에 따라 선택해야 합니다. 
Rolling Update로 시작하여 점차 복잡한 전략으로 발전시켜 나가는 것을 추천합니다.

다음 포스트에서는 Kubernetes의 Service Mesh와 Istio를 활용한 고급 트래픽 관리에 대해 알아보겠습니다!

## 참고 자료
- [Kubernetes Deployment Strategies](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy)
- [Flagger Progressive Delivery](https://flagger.app/)
- [Istio Traffic Management](https://istio.io/latest/docs/concepts/traffic-management/)
- [Nginx Ingress Canary](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary)